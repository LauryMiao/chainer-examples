{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.dataset import concat_examples\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train, test = chainer.datasets.get_mnist(withlabel=True, ndim=1, scale=1.)\n",
    "reduced_train = chainer.iterators.SerialIterator(dataset=train, batch_size=5000, repeat=True, shuffle=True).next()\n",
    "reduced_test = chainer.iterators.SerialIterator(dataset=train, batch_size=1000, repeat=True, shuffle=True).next()\n",
    "train_iter = chainer.iterators.SerialIterator(dataset=reduced_train, batch_size=batch_size, repeat=True, shuffle=True)\n",
    "test_iter = chainer.iterators.SerialIterator(dataset=reduced_test, batch_size=batch_size, repeat=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerualNetwork(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(NerualNetwork, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.fc1 = L.Linear(num_input, n_hidden_1, nobias=False,\n",
    "                               initial_bias=0)\n",
    "            self.fc2 = L.Linear(n_hidden_1, n_hidden_2)\n",
    "            self.fc3 = L.Linear(n_hidden_2, num_classes)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        z1 = F.relu(self.fc1(x))\n",
    "        z2 = F.relu(self.fc2(z1))\n",
    "        h = self.fc3(z2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NerualNetwork()\n",
    "optimiser = chainer.optimizers.SGD(lr=learning_rate)\n",
    "optimiser.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_iter_state(iterator):\n",
    "    iterator.epoch = 0\n",
    "    iterator.current_position = 0\n",
    "    iterator.is_new_epoch = False\n",
    "    iterator._pushed_position = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=002 train_loss=2.22040 val_loss=2.22141 val_accuracy=0.25879\n",
      "epoch=003 train_loss=2.12945 val_loss=2.10679 val_accuracy=0.46094\n",
      "epoch=004 train_loss=1.91256 val_loss=1.96766 val_accuracy=0.59375\n",
      "epoch=005 train_loss=1.78751 val_loss=1.80304 val_accuracy=0.66602\n",
      "epoch=006 train_loss=1.61464 val_loss=1.61648 val_accuracy=0.70703\n",
      "epoch=007 train_loss=1.43525 val_loss=1.42558 val_accuracy=0.73730\n",
      "epoch=008 train_loss=1.24614 val_loss=1.24967 val_accuracy=0.75781\n",
      "epoch=009 train_loss=1.22788 val_loss=1.09275 val_accuracy=0.78418\n",
      "epoch=010 train_loss=1.02042 val_loss=0.96866 val_accuracy=0.80176\n",
      "epoch=011 train_loss=0.84979 val_loss=0.87127 val_accuracy=0.81250\n",
      "epoch=012 train_loss=0.71199 val_loss=0.79256 val_accuracy=0.81738\n",
      "epoch=013 train_loss=0.63298 val_loss=0.73648 val_accuracy=0.83203\n",
      "epoch=014 train_loss=0.68419 val_loss=0.68782 val_accuracy=0.84082\n",
      "epoch=015 train_loss=0.64720 val_loss=0.64379 val_accuracy=0.84863\n",
      "epoch=016 train_loss=0.63590 val_loss=0.61195 val_accuracy=0.84668\n",
      "epoch=017 train_loss=0.65131 val_loss=0.57808 val_accuracy=0.85547\n",
      "epoch=018 train_loss=0.54025 val_loss=0.55624 val_accuracy=0.86035\n",
      "epoch=019 train_loss=0.55055 val_loss=0.52983 val_accuracy=0.86426\n",
      "epoch=020 train_loss=0.41112 val_loss=0.51666 val_accuracy=0.87012\n",
      "epoch=021 train_loss=0.60326 val_loss=0.49509 val_accuracy=0.87598\n",
      "epoch=022 train_loss=0.43819 val_loss=0.48169 val_accuracy=0.87891\n",
      "epoch=023 train_loss=0.46205 val_loss=0.47348 val_accuracy=0.87793\n",
      "epoch=024 train_loss=0.47603 val_loss=0.45692 val_accuracy=0.87891\n",
      "epoch=025 train_loss=0.36742 val_loss=0.44380 val_accuracy=0.88574\n",
      "epoch=026 train_loss=0.38979 val_loss=0.44082 val_accuracy=0.88477\n",
      "epoch=027 train_loss=0.33183 val_loss=0.42700 val_accuracy=0.88379\n",
      "epoch=028 train_loss=0.52222 val_loss=0.43024 val_accuracy=0.88574\n",
      "epoch=029 train_loss=0.44294 val_loss=0.41045 val_accuracy=0.88867\n",
      "epoch=030 train_loss=0.33219 val_loss=0.40475 val_accuracy=0.89160\n",
      "epoch=031 train_loss=0.40968 val_loss=0.40444 val_accuracy=0.89062\n",
      "epoch=032 train_loss=0.36038 val_loss=0.39763 val_accuracy=0.89258\n",
      "epoch=033 train_loss=0.56452 val_loss=0.38612 val_accuracy=0.89062\n",
      "epoch=034 train_loss=0.33327 val_loss=0.38913 val_accuracy=0.89258\n",
      "epoch=035 train_loss=0.28669 val_loss=0.38874 val_accuracy=0.89160\n",
      "epoch=036 train_loss=0.42428 val_loss=0.37470 val_accuracy=0.89453\n",
      "epoch=037 train_loss=0.27219 val_loss=0.36933 val_accuracy=0.89746\n",
      "epoch=038 train_loss=0.27387 val_loss=0.36518 val_accuracy=0.90137\n",
      "epoch=039 train_loss=0.32684 val_loss=0.35907 val_accuracy=0.89844\n",
      "epoch=040 train_loss=0.33328 val_loss=0.35750 val_accuracy=0.90527\n",
      "epoch=041 train_loss=0.24201 val_loss=0.35192 val_accuracy=0.90527\n",
      "epoch=042 train_loss=0.24371 val_loss=0.35431 val_accuracy=0.90527\n",
      "epoch=043 train_loss=0.37565 val_loss=0.34795 val_accuracy=0.90918\n",
      "epoch=044 train_loss=0.28631 val_loss=0.34505 val_accuracy=0.90527\n",
      "epoch=045 train_loss=0.23821 val_loss=0.34914 val_accuracy=0.90430\n",
      "epoch=046 train_loss=0.19335 val_loss=0.34647 val_accuracy=0.91113\n",
      "epoch=047 train_loss=0.25847 val_loss=0.34115 val_accuracy=0.91113\n",
      "epoch=048 train_loss=0.32170 val_loss=0.34063 val_accuracy=0.90625\n",
      "epoch=049 train_loss=0.32759 val_loss=0.33607 val_accuracy=0.91113\n",
      "epoch=050 train_loss=0.36256 val_loss=0.32878 val_accuracy=0.91309\n",
      "epoch=051 train_loss=0.26273 val_loss=0.33178 val_accuracy=0.91016\n",
      "epoch=052 train_loss=0.26836 val_loss=0.33120 val_accuracy=0.91113\n",
      "epoch=053 train_loss=0.23103 val_loss=0.32825 val_accuracy=0.90820\n",
      "epoch=054 train_loss=0.30021 val_loss=0.32380 val_accuracy=0.91309\n",
      "epoch=055 train_loss=0.36228 val_loss=0.32537 val_accuracy=0.91504\n",
      "epoch=056 train_loss=0.26072 val_loss=0.31951 val_accuracy=0.91211\n",
      "epoch=057 train_loss=0.21765 val_loss=0.31854 val_accuracy=0.91211\n",
      "epoch=058 train_loss=0.20990 val_loss=0.31246 val_accuracy=0.91504\n",
      "epoch=059 train_loss=0.23072 val_loss=0.31396 val_accuracy=0.91699\n",
      "epoch=060 train_loss=0.34545 val_loss=0.31363 val_accuracy=0.91699\n",
      "epoch=061 train_loss=0.32323 val_loss=0.30818 val_accuracy=0.91895\n",
      "epoch=062 train_loss=0.34996 val_loss=0.31844 val_accuracy=0.91797\n",
      "epoch=063 train_loss=0.20343 val_loss=0.30612 val_accuracy=0.91797\n",
      "epoch=064 train_loss=0.23222 val_loss=0.30570 val_accuracy=0.91895\n",
      "epoch=065 train_loss=0.36457 val_loss=0.30474 val_accuracy=0.91602\n",
      "epoch=066 train_loss=0.18156 val_loss=0.31643 val_accuracy=0.91602\n",
      "epoch=067 train_loss=0.22022 val_loss=0.30745 val_accuracy=0.91504\n",
      "epoch=068 train_loss=0.39350 val_loss=0.29950 val_accuracy=0.91992\n",
      "epoch=069 train_loss=0.34191 val_loss=0.30137 val_accuracy=0.91895\n",
      "epoch=070 train_loss=0.24414 val_loss=0.30125 val_accuracy=0.91895\n",
      "epoch=071 train_loss=0.25123 val_loss=0.30285 val_accuracy=0.91797\n",
      "epoch=072 train_loss=0.17418 val_loss=0.29763 val_accuracy=0.91992\n",
      "epoch=073 train_loss=0.25278 val_loss=0.29739 val_accuracy=0.92090\n",
      "epoch=074 train_loss=0.25385 val_loss=0.30307 val_accuracy=0.92188\n",
      "epoch=075 train_loss=0.31466 val_loss=0.30257 val_accuracy=0.91504\n",
      "epoch=076 train_loss=0.24472 val_loss=0.29853 val_accuracy=0.91992\n",
      "epoch=077 train_loss=0.16208 val_loss=0.29784 val_accuracy=0.91895\n",
      "epoch=078 train_loss=0.20786 val_loss=0.29147 val_accuracy=0.92090\n",
      "epoch=079 train_loss=0.25570 val_loss=0.28707 val_accuracy=0.92090\n",
      "epoch=080 train_loss=0.26593 val_loss=0.28873 val_accuracy=0.92188\n",
      "epoch=081 train_loss=0.24299 val_loss=0.28874 val_accuracy=0.92090\n",
      "epoch=082 train_loss=0.29013 val_loss=0.28266 val_accuracy=0.92383\n",
      "epoch=083 train_loss=0.26008 val_loss=0.28648 val_accuracy=0.92285\n",
      "epoch=084 train_loss=0.11094 val_loss=0.28253 val_accuracy=0.92480\n",
      "epoch=085 train_loss=0.18251 val_loss=0.28427 val_accuracy=0.92090\n",
      "epoch=086 train_loss=0.18462 val_loss=0.29363 val_accuracy=0.91895\n",
      "epoch=087 train_loss=0.14715 val_loss=0.28916 val_accuracy=0.91895\n",
      "epoch=088 train_loss=0.20613 val_loss=0.28085 val_accuracy=0.92090\n",
      "epoch=089 train_loss=0.16328 val_loss=0.28668 val_accuracy=0.92285\n",
      "epoch=090 train_loss=0.18894 val_loss=0.28057 val_accuracy=0.92285\n",
      "epoch=091 train_loss=0.18598 val_loss=0.27662 val_accuracy=0.92285\n",
      "epoch=092 train_loss=0.24256 val_loss=0.27652 val_accuracy=0.92383\n",
      "epoch=093 train_loss=0.18487 val_loss=0.27734 val_accuracy=0.92383\n",
      "epoch=094 train_loss=0.21990 val_loss=0.27941 val_accuracy=0.92480\n",
      "epoch=095 train_loss=0.21753 val_loss=0.27342 val_accuracy=0.92285\n",
      "epoch=096 train_loss=0.16262 val_loss=0.28122 val_accuracy=0.92285\n",
      "epoch=097 train_loss=0.24633 val_loss=0.27374 val_accuracy=0.92285\n",
      "epoch=098 train_loss=0.14332 val_loss=0.27176 val_accuracy=0.92285\n",
      "epoch=099 train_loss=0.25464 val_loss=0.27490 val_accuracy=0.92383\n",
      "epoch=100 train_loss=0.15958 val_loss=0.26715 val_accuracy=0.92480\n",
      "epoch=101 train_loss=0.18287 val_loss=0.26792 val_accuracy=0.92480\n"
     ]
    }
   ],
   "source": [
    "reset_iter_state(train_iter)\n",
    "\n",
    "while train_iter.epoch < training_epochs:\n",
    "    train_batch = train_iter.next()\n",
    "    image_train, target_train = concat_examples(train_batch)\n",
    "    \n",
    "    # Calculate the prediction of the network\n",
    "    prediction_train = model(image_train)\n",
    "\n",
    "    # Calculate the loss with softmax_cross_entropy\n",
    "    loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "\n",
    "    # Calculate the gradients in the network\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update all the trainable paremters\n",
    "    optimiser.update()\n",
    "    # --------------------- until here ---------------------\n",
    "\n",
    "    # Check the validation accuracy of prediction after every epoch\n",
    "    if train_iter.is_new_epoch and (train_iter.epoch + 1) % display_step == 0:\n",
    "        # If this iteration is the final iteration of the current epoch\n",
    "\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "            test_batch = test_iter.next()\n",
    "            image_test, target_test = concat_examples(test_batch)\n",
    "\n",
    "            # Forward the test data\n",
    "            prediction_test = model(image_test)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
    "            test_losses.append(loss_test.data)\n",
    "\n",
    "            # Calculate the accuracy\n",
    "            accuracy = F.accuracy(prediction_test, target_test)\n",
    "            test_accuracies.append(accuracy.data)\n",
    "\n",
    "            if test_iter.is_new_epoch:\n",
    "                reset_iter_state(test_iter)\n",
    "                break\n",
    "\n",
    "        print('epoch={:03d} train_loss={:.05f} val_loss={:.05f} val_accuracy={:.05f}'.format(\n",
    "            train_iter.epoch + 1, float(loss.data), np.mean(test_losses), np.mean(test_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
